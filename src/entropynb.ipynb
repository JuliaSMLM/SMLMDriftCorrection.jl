{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy Upper Bound from SMLM Data\n",
    "From Carlas Smith Drift Correction Paper:\n",
    "[Drift correction in localization microscopy using entropy minimization](https://opg.optica.org/oe/fulltext.cfm?uri=oe-29-18-27961&id=457245)\n",
    "\n",
    "Note: This journal lets you grab the latex for formulas, which I just pasted here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H(\\pmb{D}) \\leq U_H(\\pmb{D})= \\sum_{i=1}^{N} H_i(\\pmb{D}) - \\frac{1}{N} \\sum_{i=1}^{N} \\log \\left( \\sum_{j=1}^{N} \\frac{1}{N} e^{{-}D_{\\textrm{KL}}\\left(p_i(\\pmb{r}) || p_j(\\pmb{r})\\right) } \\right)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$D_{\\textrm{KL}}\\left(p_i(\\pmb{r}) || p_j(\\pmb{r})\\right) = \\frac{1}{2} \\sum_{k=1}^{K} \\left( \\log \\left( \\frac{\\sigma_{j,k}^2}{\\sigma_{i,k}^2} \\right) + \\frac{\\sigma_{i,k}^2}{\\sigma_{j,k}^2} + \\frac{\\left( \\mu_{i,k} - d_k(t_i) - \\mu_{j,k} + d_k(t_j) \\right)^2}{\\sigma_{j,k}^2} \\right) - \\frac{K}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Gaussian mixture model (GMM), entropy is a measure of the uncertainty or randomness of a given distribution. In the context of GMMs, entropy can be defined as the sum of the entropies of each individual Gaussian component in the mixture.\n",
    "\n",
    "The entropy of a single Gaussian distribution with mean $\\mu$ and covariance matrix $\\Sigma$ can be calculated using the following formula:\n",
    "\n",
    "$H(\\mathcal{N}(\\mu, \\Sigma)) = \\frac{1}{2} \\ln \\det(2\\pi e \\Sigma)$\n",
    "\n",
    "Where $\\det(\\cdot)$ is the determinant of a matrix and $\\mathcal{N}(\\mu, \\Sigma)$ represents a Gaussian distribution with mean $\\mu$ and covariance matrix $\\Sigma$.\n",
    "\n",
    "The entropy of a GMM is then calculated as a weighted sum of the entropies of each individual Gaussian component in the mixture, where the weights represent the mixture proportions:\n",
    "\n",
    "$H(\\text{GMM}) = \\sum_{i=1}^{K} w_i H(\\mathcal{N}(\\mu_i, \\Sigma_i))$\n",
    "\n",
    "Where $K$ is the number of Gaussian components in the mixture, $w_i$ is the mixture proportion for the $i$-th component, $\\mu_i$ and $\\Sigma_i$ are the mean and covariance matrix for the $i$-th component, respectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
